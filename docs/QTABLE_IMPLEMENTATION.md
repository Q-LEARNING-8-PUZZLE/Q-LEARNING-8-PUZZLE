# Implementaci√≥n de la Tabla Q (Q-Table)

## üìã Descripci√≥n General

La **Tabla Q** es la estructura de datos fundamental del algoritmo Q-Learning. Almacena los valores Q que representan la calidad esperada de tomar una acci√≥n espec√≠fica en un estado dado. Esta implementaci√≥n est√° dise√±ada para manejar eficientemente la gran cantidad de estados posibles del 8-Puzzle (aproximadamente 181,440 estados solucionables).

## üéØ Objetivo

Crear una estructura de datos eficiente que mapee `(Estado, Acci√≥n) ‚Üí Valor Q`, permitiendo:
- Almacenar valores Q de forma eficiente
- Inicializar valores autom√°ticamente cuando se accede a nuevos pares (estado, acci√≥n)
- Obtener y actualizar valores Q f√°cilmente
- Encontrar la mejor acci√≥n para un estado dado

## üèóÔ∏è Arquitectura de la Implementaci√≥n

### Estructura de Datos

La implementaci√≥n utiliza un **diccionario de Python** (`defaultdict`) con la siguiente estructura:

```python
{
    (estado, acci√≥n): valor_q
}
```

Donde:
- **Estado**: Tupla inmutable de 9 enteros que representa la configuraci√≥n del tablero
  - Ejemplo: `(1, 2, 3, 4, 5, 6, 7, 8, 0)`
- **Acci√≥n**: Entero que representa la acci√≥n (0-3)
  - `0`: Mover espacio vac√≠o ARRIBA
  - `1`: Mover espacio vac√≠o ABAJO
  - `2`: Mover espacio vac√≠o IZQUIERDA
  - `3`: Mover espacio vac√≠o DERECHA
- **Valor Q**: N√∫mero flotante que representa la calidad esperada de la acci√≥n

### Ejemplo Visual

```
Tabla Q:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ (Estado, Acci√≥n)                   ‚îÇ Valor Q ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ((1,2,3,4,5,6,7,8,0), 0)          ‚îÇ  0.5    ‚îÇ
‚îÇ ((1,2,3,4,5,6,7,8,0), 1)          ‚îÇ  0.3    ‚îÇ
‚îÇ ((2,1,3,4,5,6,7,8,0), 0)          ‚îÇ  0.1    ‚îÇ
‚îÇ ((2,1,3,4,5,6,7,8,0), 2)          ‚îÇ  0.7    ‚îÇ
‚îÇ ...                                 ‚îÇ  ...    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìù Componentes Principales

### 1. Inicializaci√≥n (`__init__`)

```python
def __init__(self, initial_value: float = 0.0):
    self._q_table = defaultdict(lambda: initial_value)
    self.initial_value = initial_value
```

**Caracter√≠sticas:**
- Usa `defaultdict` para inicializaci√≥n autom√°tica (lazy initialization)
- No necesita pre-inicializar todos los estados posibles
- Valor inicial por defecto: `0.0` (puede cambiarse)
- Eficiente en memoria: solo almacena estados visitados

**Ventajas:**
- ‚úÖ No requiere memoria para estados nunca visitados
- ‚úÖ Inicializaci√≥n autom√°tica al acceder a nuevos pares
- ‚úÖ Flexible: permite cambiar el valor inicial

### 2. Obtener Valor Q (`get`)

```python
def get(self, state: Tuple[int, ...], action: int) -> float:
    return self._q_table[(state, action)]
```

**Funcionalidad:**
- Retorna el valor Q para un par (estado, acci√≥n)
- Si el par no existe, `defaultdict` retorna autom√°ticamente `initial_value`
- Operaci√≥n O(1) en promedio (hash map)

**Ejemplo:**
```python
valor = q_table.get((1, 2, 3, 4, 5, 6, 7, 8, 0), 0)
# Si no existe: retorna 0.0 (valor inicial)
# Si existe: retorna el valor almacenado
```

### 3. Establecer Valor Q (`set`)

```python
def set(self, state: Tuple[int, ...], action: int, value: float) -> None:
    self._q_table[(state, action)] = value
```

**Funcionalidad:**
- Establece o actualiza el valor Q para un par (estado, acci√≥n)
- Operaci√≥n O(1) en promedio

**Ejemplo:**
```python
q_table.set((1, 2, 3, 4, 5, 6, 7, 8, 0), 0, 0.5)
# Ahora Q((1,2,3,4,5,6,7,8,0), 0) = 0.5
```

### 4. Obtener Mejor Acci√≥n (`get_best_action`)

```python
def get_best_action(self, state: Tuple[int, ...], valid_actions: list[int]) -> Optional[int]:
    # Encuentra la acci√≥n con mayor valor Q entre las acciones v√°lidas
    ...
```

**Funcionalidad:**
- Encuentra la acci√≥n con el mayor valor Q para un estado dado
- Solo considera acciones v√°lidas (que no violan l√≠mites del tablero)
- Retorna `None` si no hay acciones v√°lidas

**Uso en Q-Learning:**
- Utilizado durante la **explotaci√≥n** (cuando epsilon-greedy elige la mejor acci√≥n conocida)
- Parte esencial de la pol√≠tica epsilon-greedy

**Ejemplo:**
```python
state = (1, 2, 3, 4, 5, 6, 7, 8, 0)
valid_actions = [1, 3]  # Solo puede mover abajo o derecha
mejor_accion = q_table.get_best_action(state, valid_actions)
# Retorna la acci√≥n con mayor Q entre [1, 3]
```

### 5. Obtener M√°ximo Valor Q (`get_max_q_value`)

```python
def get_max_q_value(self, state: Tuple[int, ...], valid_actions: list[int]) -> float:
    # Retorna el m√°ximo valor Q entre las acciones v√°lidas
    ...
```

**Funcionalidad:**
- Retorna el m√°ximo valor Q para un estado dado
- Esencial para la ecuaci√≥n de actualizaci√≥n Q-Learning

**Uso en Q-Learning:**
- Necesario para calcular `max Q(s', a')` en la ecuaci√≥n:
  ```
  Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ * max Q(s',a') - Q(s,a)]
  ```

**Ejemplo:**
```python
next_state = (2, 1, 3, 4, 5, 6, 7, 8, 0)
next_valid_actions = [0, 1, 2, 3]
max_q = q_table.get_max_q_value(next_state, next_valid_actions)
# Usado en la actualizaci√≥n Q-Learning
```

### 6. M√©todos Auxiliares

#### `size()` / `__len__()`
Retorna el n√∫mero de pares (estado, acci√≥n) almacenados en la tabla.

```python
tama√±o = q_table.size()  # o len(q_table)
```

#### `clear()`
Limpia toda la tabla Q, eliminando todas las entradas.

```python
q_table.clear()
```

#### `__repr__()`
Representaci√≥n legible de la tabla Q.

```python
print(q_table)  # QTable(size=1234, initial_value=0.0)
```

## üîÑ Flujo de Uso en Q-Learning

### 1. Inicializaci√≥n
```python
from app.agent import QTable, QLearningAgent

q_table = QTable(initial_value=0.0)
agent = QLearningAgent(q_table, epsilon=0.1, alpha=0.1, gamma=0.9)
```

### 2. Durante el Entrenamiento (Usando QLearningAgent)

```python
# Paso 1: Obtener estado actual
state = env.state

# Paso 2: Obtener acciones v√°lidas
valid_actions = env.get_valid_actions(state)

# Paso 3: Elegir acci√≥n usando epsilon-greedy (TASK-05)
action = agent.choose_action(state, valid_actions)

# Paso 4: Ejecutar acci√≥n y obtener recompensa
next_state, action_valid = env.step(action)
reward = env.get_reward(next_state, action_valid)

# Paso 5: Obtener acciones v√°lidas del siguiente estado
next_valid_actions = env.get_valid_actions(next_state)

# Paso 6: Actualizar Q usando la ecuaci√≥n Q-Learning (TASK-06)
agent.update(state, action, reward, next_state, next_valid_actions)
```

**Nota**: Para uso manual sin `QLearningAgent`, ver la secci√≥n anterior. Sin embargo, se recomienda usar `QLearningAgent` para c√≥digo m√°s limpio y mantenible.

## ‚ö° Optimizaciones y Consideraciones

### 1. Lazy Initialization
- **No pre-inicializa** todos los estados posibles
- Solo crea entradas cuando se accede a ellas
- Ahorra memoria significativamente

### 2. Uso de Tuplas Inmutables
- Los estados son tuplas inmutables ‚Üí pueden usarse como claves de diccionario
- Hash eficiente y seguro
- Comparaci√≥n r√°pida

### 3. defaultdict vs dict normal
- **defaultdict**: Inicializaci√≥n autom√°tica
- **dict normal**: Requerir√≠a verificar existencia antes de acceder
- `defaultdict` simplifica el c√≥digo y mejora rendimiento

### 4. Escalabilidad
- Maneja eficientemente ~181,440 estados solucionables
- Cada entrada ocupa aproximadamente:
  - Clave (tupla + int): ~100 bytes
  - Valor (float): 8 bytes
  - Overhead de dict: ~50 bytes
  - **Total por entrada**: ~158 bytes
- **Memoria estimada m√°xima**: ~28 MB (muy manejable)

## üìä Comparaci√≥n con Alternativas

| M√©todo | Ventajas | Desventajas |
|--------|----------|-------------|
| **Diccionario (implementaci√≥n actual)** | ‚úÖ Eficiente O(1) acceso<br>‚úÖ Lazy initialization<br>‚úÖ Flexible | ‚ö†Ô∏è Overhead de hash |
| **Array/Matriz** | ‚úÖ Acceso r√°pido | ‚ùå Requiere indexaci√≥n compleja<br>‚ùå Memoria fija grande<br>‚ùå Dif√≠cil de implementar |
| **Base de datos** | ‚úÖ Persistencia | ‚ùå Muy lento<br>‚ùå Overhead innecesario |

## ‚úÖ Criterios de Aceptaci√≥n Cumplidos

### ‚úÖ Estructura Eficiente
- Utiliza diccionario/hash map con tuplas como claves
- Acceso O(1) en promedio
- Lazy initialization para eficiencia de memoria

### ‚úÖ Inicializaci√≥n Correcta
- Valores inicializados autom√°ticamente con `defaultdict`
- Valor inicial configurable
- Manejo correcto de estados no visitados

## üß™ Ejemplo de Uso Completo

```python
from app.agent import QTable
from app.environment import EightPuzzle

# Crear tabla Q
q_table = QTable(initial_value=0.0)

# Crear entorno
env = EightPuzzle()
env.reset()

# Estado inicial
state = env.state
print(f"Estado inicial: {state}")

# Obtener acciones v√°lidas
valid_actions = env.get_valid_actions(state)
print(f"Acciones v√°lidas: {valid_actions}")

# Obtener valores Q iniciales (todos 0.0)
for action in valid_actions:
    q_value = q_table.get(state, action)
    print(f"Q({state}, {action}) = {q_value}")

# Establecer algunos valores Q
q_table.set(state, 0, 0.5)
q_table.set(state, 1, 0.3)
q_table.set(state, 2, 0.7)
q_table.set(state, 3, 0.2)

# Obtener mejor acci√≥n
best_action = q_table.get_best_action(state, valid_actions)
print(f"Mejor acci√≥n: {best_action}")

# Obtener m√°ximo valor Q
max_q = q_table.get_max_q_value(state, valid_actions)
print(f"M√°ximo Q: {max_q}")

# Ver tama√±o de la tabla
print(f"Tama√±o de tabla: {q_table.size()}")
print(f"Representaci√≥n: {q_table}")
```

## üìö Referencias

- **Q-Learning Algorithm**: Algoritmo de aprendizaje por refuerzo que aprende la funci√≥n Q
- **8-Puzzle Problem**: Problema de b√∫squeda cl√°sico con ~181,440 estados solucionables
- **Hash Maps**: Estructura de datos eficiente para b√∫squeda O(1)
- **Lazy Initialization**: Patr√≥n de dise√±o que inicializa valores solo cuando se necesitan

## üîó Relaci√≥n con Otras Tareas

- **TASK-05**: ‚úÖ Implementaci√≥n de Pol√≠tica Epsilon-Greedy (usa `get_best_action`)
  - Ver documentaci√≥n completa en `docs/AGENT_IMPLEMENTATION.md`
- **TASK-06**: ‚úÖ Implementaci√≥n de la Ecuaci√≥n de Actualizaci√≥n Q (usa `get`, `set`, `get_max_q_value`)
  - Ver documentaci√≥n completa en `docs/AGENT_IMPLEMENTATION.md`
- **TASK-03**: Generaci√≥n de Tabla de Transiciones (provee estados alcanzables)

---

**Autor**: Dev 2  
**Tarea**: TASK-04 - Implementaci√≥n de la Tabla Q (Q-Table)  
**Fecha**: 2024

