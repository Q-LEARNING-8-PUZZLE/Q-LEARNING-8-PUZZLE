# Implementaci√≥n del Agente Q-Learning

## üìã Descripci√≥n General

Este documento describe la implementaci√≥n de las tareas **TASK-05** (Pol√≠tica Epsilon-Greedy) y **TASK-06** (Ecuaci√≥n de Actualizaci√≥n Q) del agente Q-Learning para el problema del 8-Puzzle.

## üéØ Objetivos

### TASK-05: Pol√≠tica Epsilon-Greedy
Implementar la l√≥gica de selecci√≥n de acciones que equilibre la exploraci√≥n y explotaci√≥n:
- Elegir una acci√≥n aleatoria con probabilidad `epsilon` (exploraci√≥n)
- Elegir la mejor acci√≥n conocida con probabilidad `1 - epsilon` (explotaci√≥n)
- `epsilon` debe ser parametrizable

### TASK-06: Ecuaci√≥n de Actualizaci√≥n Q
Implementar la f√≥rmula de actualizaci√≥n Q-Learning:
```
Q(s,a) = Q(s,a) + alpha * [r + gamma * max(Q(s',a')) - Q(s,a)]
```
- Funci√≥n `update()` correcta
- Uso correcto de `alpha` (tasa de aprendizaje) y `gamma` (factor de descuento)

## üèóÔ∏è Arquitectura de la Implementaci√≥n

### Clase QLearningAgent

La clase `QLearningAgent` encapsula toda la l√≥gica del agente Q-Learning, incluyendo:
- Pol√≠tica de selecci√≥n de acciones (epsilon-greedy)
- Actualizaci√≥n de valores Q
- Gesti√≥n de hiperpar√°metros (epsilon, alpha, gamma)

```python
class QLearningAgent:
    def __init__(self, q_table, epsilon=0.1, alpha=0.1, gamma=0.9)
    def choose_action(self, state, valid_actions) -> int
    def update(self, state, action, reward, next_state, next_valid_actions) -> None
    def set_epsilon(self, epsilon: float) -> None
    def set_alpha(self, alpha: float) -> None
    def set_gamma(self, gamma: float) -> None
```

## üìù Componentes Principales

### 1. Inicializaci√≥n (`__init__`)

```python
def __init__(
    self,
    q_table: QTable,
    epsilon: float = 0.1,
    alpha: float = 0.1,
    gamma: float = 0.9
):
    self.q_table = q_table
    self.epsilon = epsilon
    self.alpha = alpha
    self.gamma = gamma
```

**Par√°metros:**
- `q_table`: Instancia de `QTable` para almacenar valores Q
- `epsilon`: Probabilidad de exploraci√≥n (0.0-1.0)
- `alpha`: Tasa de aprendizaje (0.0-1.0)
- `gamma`: Factor de descuento (0.0-1.0)

**Caracter√≠sticas:**
- ‚úÖ Todos los hiperpar√°metros son configurables
- ‚úÖ Valores por defecto razonables para comenzar el entrenamiento
- ‚úÖ Integraci√≥n con la tabla Q existente

### 2. TASK-05: Selecci√≥n de Acci√≥n (`choose_action`)

#### Implementaci√≥n

```python
def choose_action(
    self,
    state: Tuple[int, ...],
    valid_actions: List[int]
) -> int:
    if not valid_actions:
        raise ValueError("No hay acciones v√°lidas disponibles")
    
    # Exploraci√≥n: elegir acci√≥n aleatoria con probabilidad epsilon
    if random.random() < self.epsilon:
        return random.choice(valid_actions)
    
    # Explotaci√≥n: elegir la mejor acci√≥n conocida
    best_action = self.q_table.get_best_action(state, valid_actions)
    best_value = self.q_table.get(state, best_action)
    
    # Manejar empates eligiendo aleatoriamente entre las mejores
    best_actions = [
        action for action in valid_actions
        if self.q_table.get(state, action) == best_value
    ]
    
    return random.choice(best_actions)
```

#### Algoritmo Paso a Paso

1. **Validaci√≥n de Entrada**
   - Verificar que hay acciones v√°lidas disponibles
   - Lanzar excepci√≥n si no hay acciones v√°lidas

2. **Decisi√≥n de Exploraci√≥n/Explotaci√≥n**
   - Generar n√∫mero aleatorio entre 0 y 1
   - Si `random() < epsilon` ‚Üí **Exploraci√≥n**
   - Si `random() >= epsilon` ‚Üí **Explotaci√≥n**

3. **Exploraci√≥n (Acci√≥n Aleatoria)**
   - Elegir aleatoriamente una acci√≥n de `valid_actions`
   - Permite descubrir nuevas estrategias
   - Importante al inicio del entrenamiento

4. **Explotaci√≥n (Mejor Acci√≥n Conocida)**
   - Obtener la mejor acci√≥n usando `q_table.get_best_action()`
   - Obtener el valor Q de la mejor acci√≥n
   - Identificar todas las acciones con el mismo valor Q (empates)
   - Elegir aleatoriamente entre las acciones empatadas

#### Ejemplo Visual

```
Estado: (1, 2, 3, 4, 5, 6, 7, 0, 8)
Acciones v√°lidas: [0, 1, 2, 3]
Epsilon: 0.1

Valores Q actuales:
  Q(state, 0) = 0.5
  Q(state, 1) = 0.3
  Q(state, 2) = 0.7  ‚Üê Mejor
  Q(state, 3) = 0.2

Decisi√≥n:
  random() = 0.15
  
  Como 0.15 >= 0.1 (epsilon):
    ‚Üí Explotaci√≥n
    ‚Üí Elegir acci√≥n 2 (mejor valor Q = 0.7)
```

#### Manejo de Empates

Cuando m√∫ltiples acciones tienen el mismo valor Q m√°ximo, el algoritmo elige aleatoriamente entre ellas. Esto evita sesgos y mejora la exploraci√≥n incluso durante la explotaci√≥n.

**Ejemplo de Empate:**
```python
Q(state, 0) = 0.5
Q(state, 1) = 0.5  ‚Üê Empate
Q(state, 2) = 0.3
Q(state, 3) = 0.5  ‚Üê Empate

# Elige aleatoriamente entre [0, 1, 3]
```

### 3. TASK-06: Actualizaci√≥n Q (`update`)

#### Implementaci√≥n

```python
def update(
    self,
    state: Tuple[int, ...],
    action: int,
    reward: float,
    next_state: Tuple[int, ...],
    next_valid_actions: List[int]
) -> None:
    # Obtener el valor Q actual
    current_q_value = self.q_table.get(state, action)
    
    # Calcular el m√°ximo valor Q para el siguiente estado
    max_next_q_value = self.q_table.get_max_q_value(
        next_state, 
        next_valid_actions
    )
    
    # Calcular el valor objetivo (target value)
    target_value = reward + self.gamma * max_next_q_value
    
    # Aplicar la ecuaci√≥n de actualizaci√≥n Q-Learning
    new_q_value = current_q_value + self.alpha * (
        target_value - current_q_value
    )
    
    # Actualizar la tabla Q
    self.q_table.set(state, action, new_q_value)
```

#### Ecuaci√≥n de Actualizaci√≥n Q-Learning

La f√≥rmula implementada es:

```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ * max Q(s',a') - Q(s,a)]
```

Donde:
- `s`: Estado actual
- `a`: Acci√≥n tomada
- `r`: Recompensa recibida
- `s'`: Estado siguiente
- `Œ±` (alpha): Tasa de aprendizaje
- `Œ≥` (gamma): Factor de descuento
- `max Q(s',a')`: M√°ximo valor Q del siguiente estado

#### Algoritmo Paso a Paso

1. **Obtener Valor Q Actual**
   ```python
   current_q_value = self.q_table.get(state, action)
   ```
   - Recupera el valor Q almacenado para el par (estado, acci√≥n)
   - Si no existe, retorna el valor inicial (0.0 por defecto)

2. **Calcular M√°ximo Q del Siguiente Estado**
   ```python
   max_next_q_value = self.q_table.get_max_q_value(
       next_state, 
       next_valid_actions
   )
   ```
   - Encuentra el m√°ximo valor Q entre todas las acciones v√°lidas del siguiente estado
   - Representa la mejor recompensa futura esperada

3. **Calcular Valor Objetivo (Target)**
   ```python
   target_value = reward + self.gamma * max_next_q_value
   ```
   - Combina la recompensa inmediata con la recompensa futura descontada
   - `gamma` controla qu√© tan importante es el futuro (0 = solo presente, 1 = futuro igual de importante)

4. **Aplicar Actualizaci√≥n**
   ```python
   new_q_value = current_q_value + self.alpha * (
       target_value - current_q_value
   )
   ```
   - Calcula la diferencia entre el valor objetivo y el valor actual
   - `alpha` controla qu√© tan r√°pido se actualiza el valor Q
   - Actualizaci√≥n incremental (no reemplaza completamente el valor anterior)

5. **Guardar Nuevo Valor**
   ```python
   self.q_table.set(state, action, new_q_value)
   ```
   - Almacena el nuevo valor Q en la tabla

#### Ejemplo Num√©rico

```python
# Estado inicial
state = (1, 2, 3, 4, 5, 6, 7, 0, 8)
action = 0  # ARRIBA
reward = -1.0
next_state = (1, 2, 3, 4, 0, 6, 7, 5, 8)
next_valid_actions = [0, 1, 2, 3]

# Hiperpar√°metros
alpha = 0.1
gamma = 0.9

# Valores Q iniciales (todos 0.0)
current_q = 0.0
max_next_q = 0.0  # Todas las acciones tienen Q = 0.0

# C√°lculo
target_value = -1.0 + 0.9 * 0.0 = -1.0
new_q = 0.0 + 0.1 * (-1.0 - 0.0) = -0.1

# Resultado: Q(state, action) = -0.1
```

#### Interpretaci√≥n de Hiperpar√°metros

**Alpha (Œ±) - Tasa de Aprendizaje:**
- `alpha = 0.0`: No aprende (mantiene valores iniciales)
- `alpha = 0.1`: Aprendizaje conservador (cambios graduales)
- `alpha = 1.0`: Aprendizaje agresivo (reemplaza completamente el valor anterior)
- **Recomendado**: 0.1 - 0.3 para problemas estoc√°sticos

**Gamma (Œ≥) - Factor de Descuento:**
- `gamma = 0.0`: Solo considera recompensas inmediatas
- `gamma = 0.9`: Considera recompensas futuras con descuento moderado
- `gamma = 1.0`: Recompensas futuras igual de importantes que las inmediatas
- **Recomendado**: 0.9 - 0.99 para problemas secuenciales

### 4. M√©todos de Configuraci√≥n de Hiperpar√°metros

#### `set_epsilon(epsilon: float)`

```python
def set_epsilon(self, epsilon: float) -> None:
    if not 0.0 <= epsilon <= 1.0:
        raise ValueError("epsilon debe estar entre 0.0 y 1.0")
    self.epsilon = epsilon
```

**Uso:**
- √ötil para implementar decaimiento de epsilon durante el entrenamiento
- Permite comenzar con alta exploraci√≥n y reducir gradualmente

**Ejemplo de Decaimiento:**
```python
# Inicio: alta exploraci√≥n
agent.set_epsilon(1.0)

# Durante entrenamiento: reducir gradualmente
for episode in range(num_episodes):
    epsilon = max(0.01, 1.0 - episode / num_episodes)
    agent.set_epsilon(epsilon)
```

#### `set_alpha(alpha: float)`

```python
def set_alpha(self, alpha: float) -> None:
    if not 0.0 <= alpha <= 1.0:
        raise ValueError("alpha debe estar entre 0.0 y 1.0")
    self.alpha = alpha
```

**Uso:**
- Permite ajustar la tasa de aprendizaje din√°micamente
- √ötil para t√©cnicas como annealing de tasa de aprendizaje

#### `set_gamma(gamma: float)`

```python
def set_gamma(self, gamma: float) -> None:
    if not 0.0 <= gamma <= 1.0:
        raise ValueError("gamma debe estar entre 0.0 y 1.0")
    self.gamma = gamma
```

**Uso:**
- Generalmente se mantiene constante durante el entrenamiento
- Puede ajustarse seg√∫n el horizonte del problema

## üîÑ Flujo Completo de Uso

### Ejemplo de Uso B√°sico

```python
from app.agent import QTable, QLearningAgent
from app.environment import EightPuzzle

# 1. Inicializar componentes
q_table = QTable(initial_value=0.0)
agent = QLearningAgent(
    q_table=q_table,
    epsilon=0.1,
    alpha=0.1,
    gamma=0.9
)
env = EightPuzzle()

# 2. Resetear entorno
state = env.reset(random_start=True)
done = False
steps = 0

# 3. Bucle de entrenamiento (un episodio)
while not done and steps < 1000:
    # Obtener acciones v√°lidas
    valid_actions = env.get_valid_actions(state)
    
    # Elegir acci√≥n usando epsilon-greedy
    action = agent.choose_action(state, valid_actions)
    
    # Ejecutar acci√≥n
    next_state, action_valid = env.step(action)
    reward = env.get_reward(next_state, action_valid)
    
    # Obtener acciones v√°lidas del siguiente estado
    next_valid_actions = env.get_valid_actions(next_state)
    
    # Actualizar Q
    agent.update(
        state, 
        action, 
        reward, 
        next_state, 
        next_valid_actions
    )
    
    # Verificar si termin√≥
    done = env.is_goal(next_state)
    
    # Actualizar estado
    state = next_state
    steps += 1

print(f"Episodio completado en {steps} pasos")
```

### Flujo Detallado Paso a Paso

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. INICIALIZACI√ìN                                       ‚îÇ
‚îÇ    - Crear QTable                                       ‚îÇ
‚îÇ    - Crear QLearningAgent con hiperpar√°metros         ‚îÇ
‚îÇ    - Crear Environment                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. RESET DEL ENTORNO                                    ‚îÇ
‚îÇ    - Generar estado inicial aleatorio                   ‚îÇ
‚îÇ    - done = False                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. BUCLE DE ENTRENAMIENTO (por cada paso)               ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ    a) Obtener acciones v√°lidas                         ‚îÇ
‚îÇ       valid_actions = env.get_valid_actions(state)     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ    b) Elegir acci√≥n (TASK-05)                          ‚îÇ
‚îÇ       action = agent.choose_action(state, valid_actions)‚îÇ
‚îÇ       ‚îú‚îÄ Con prob. epsilon: acci√≥n aleatoria           ‚îÇ
‚îÇ       ‚îî‚îÄ Con prob. (1-epsilon): mejor acci√≥n conocida  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ    c) Ejecutar acci√≥n                                   ‚îÇ
‚îÇ       next_state, valid = env.step(action)              ‚îÇ
‚îÇ       reward = env.get_reward(next_state, valid)       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ    d) Actualizar Q (TASK-06)                           ‚îÇ
‚îÇ       agent.update(state, action, reward,              ‚îÇ
‚îÇ                    next_state, next_valid_actions)     ‚îÇ
‚îÇ       ‚îú‚îÄ Obtener Q(s,a) actual                         ‚îÇ
‚îÇ       ‚îú‚îÄ Calcular max Q(s',a')                         ‚îÇ
‚îÇ       ‚îú‚îÄ Calcular target = r + Œ≥*max Q(s',a')          ‚îÇ
‚îÇ       ‚îú‚îÄ Actualizar: Q(s,a) += Œ±*(target - Q(s,a))    ‚îÇ
‚îÇ       ‚îî‚îÄ Guardar nuevo Q(s,a)                          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ    e) Verificar terminaci√≥n                             ‚îÇ
‚îÇ       done = env.is_goal(next_state)                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ    f) Actualizar estado                                ‚îÇ
‚îÇ       state = next_state                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. FIN DEL EPISODIO                                     ‚îÇ
‚îÇ    - Guardar m√©tricas (pasos, recompensa total)        ‚îÇ
‚îÇ    - Repetir desde paso 2 para nuevo episodio          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üß™ Pruebas y Validaci√≥n

### Script de Prueba Incluido

El archivo `app/agent.py` incluye un bloque de prueba que valida ambas funcionalidades:

```python
if __name__ == "__main__":
    # Prueba TASK-05: choose_action
    # Prueba TASK-06: update
    # Verificaci√≥n manual de la f√≥rmula
```

### Ejecutar Pruebas

```bash
python app/agent.py
```

### Resultados Esperados

```
============================================================
Pruebas de TASK-05 y TASK-06
============================================================

--- TASK-05: Prueba de Pol√≠tica Epsilon-Greedy ---
Estado inicial: (7, 1, 6, 2, 0, 3, 5, 8, 4)
Acciones v√°lidas: [0, 1, 2, 3]
Epsilon: 0.1
‚úì choose_action funciona correctamente

--- TASK-06: Prueba de Actualizaci√≥n Q ---
Estado actual: (1, 2, 3, 4, 5, 6, 7, 0, 8)
Acci√≥n tomada: 0
Recompensa: -1.0
Estado siguiente: (1, 2, 3, 4, 0, 6, 7, 5, 8)
‚úì La actualizaci√≥n Q funciona correctamente
‚úì La f√≥rmula de actualizaci√≥n es correcta

============================================================
Pruebas completadas
============================================================
```

## ‚ö° Optimizaciones y Consideraciones

### 1. Manejo de Empates en choose_action

- Cuando m√∫ltiples acciones tienen el mismo valor Q m√°ximo, se elige aleatoriamente entre ellas
- Evita sesgos hacia acciones espec√≠ficas
- Mejora la exploraci√≥n incluso durante la explotaci√≥n

### 2. Validaci√≥n de Par√°metros

- Todos los m√©todos de configuraci√≥n validan que los valores est√©n en el rango [0.0, 1.0]
- Previene errores por valores inv√°lidos
- Mensajes de error claros

### 3. Integraci√≥n con QTable

- Usa los m√©todos existentes de `QTable` (`get`, `set`, `get_best_action`, `get_max_q_value`)
- No duplica l√≥gica
- Mantiene la separaci√≥n de responsabilidades

### 4. Eficiencia

- `choose_action`: O(n) donde n es el n√∫mero de acciones v√°lidas (m√°ximo 4)
- `update`: O(n) donde n es el n√∫mero de acciones v√°lidas del siguiente estado
- Operaciones muy r√°pidas para el problema del 8-Puzzle

## üìä Comparaci√≥n con Alternativas

| Aspecto | Implementaci√≥n Actual | Alternativa Manual |
|---------|---------------------|-------------------|
| **C√≥digo** | Encapsulado en clase | Disperso en m√∫ltiples lugares |
| **Reutilizaci√≥n** | ‚úÖ F√°cil de reutilizar | ‚ùå Dif√≠cil de mantener |
| **Testing** | ‚úÖ F√°cil de probar | ‚ùå Dif√≠cil de aislar |
| **Configuraci√≥n** | ‚úÖ M√©todos dedicados | ‚ùå Variables globales |
| **Mantenibilidad** | ‚úÖ C√≥digo organizado | ‚ùå C√≥digo acoplado |

## ‚úÖ Criterios de Aceptaci√≥n Cumplidos

### TASK-05: Pol√≠tica Epsilon-Greedy

‚úÖ **Funci√≥n `choose_action(state)` funcional**
- Implementada en `QLearningAgent.choose_action()`
- Maneja correctamente exploraci√≥n y explotaci√≥n
- Maneja empates correctamente

‚úÖ **`epsilon` debe ser parametrizable**
- Configurable en el constructor
- M√©todo `set_epsilon()` para cambios din√°micos
- Validaci√≥n de rango [0.0, 1.0]

### TASK-06: Ecuaci√≥n de Actualizaci√≥n Q

‚úÖ **Funci√≥n `update(state, action, reward, next_state)` correcta**
- Implementada en `QLearningAgent.update()`
- Implementa la f√≥rmula completa de Q-Learning
- Maneja correctamente estados terminales

‚úÖ **Uso correcto de `alpha` y `gamma`**
- `alpha` controla la tasa de actualizaci√≥n
- `gamma` controla el descuento de recompensas futuras
- Ambos son configurables y validados

## üîó Relaci√≥n con Otras Tareas

- **TASK-04**: Usa `QTable` para almacenar y recuperar valores Q
- **TASK-07**: Usa el sistema de recompensas del entorno
- **TASK-08**: Ser√° usado en el bucle principal de entrenamiento
- **TASK-09**: Los hiperpar√°metros pueden ajustarse din√°micamente

## üìö Referencias

- **Q-Learning Algorithm**: Algoritmo de aprendizaje por refuerzo sin modelo
- **Epsilon-Greedy Policy**: Estrategia de balance exploraci√≥n/explotaci√≥n
- **Temporal Difference Learning**: M√©todo de actualizaci√≥n incremental
- **8-Puzzle Problem**: Problema de b√∫squeda cl√°sico

## üéì Conceptos Clave

### Exploraci√≥n vs Explotaci√≥n

- **Exploraci√≥n**: Probar acciones nuevas para descubrir mejores estrategias
- **Explotaci√≥n**: Usar el conocimiento actual para maximizar recompensas
- **Balance**: Epsilon-greedy equilibra ambos mediante probabilidad

### Aprendizaje Incremental

- Los valores Q se actualizan gradualmente, no se reemplazan completamente
- `alpha` controla qu√© tan r√°pido se incorpora nueva informaci√≥n
- Permite adaptaci√≥n continua durante el entrenamiento

### Descuento Temporal

- `gamma` determina qu√© tan importante es el futuro
- Valores altos de gamma ‚Üí planificaci√≥n a largo plazo
- Valores bajos de gamma ‚Üí enfoque en recompensas inmediatas

---

**Autor**: Dev 2  
**Tareas**: TASK-05 - Pol√≠tica Epsilon-Greedy | TASK-06 - Ecuaci√≥n de Actualizaci√≥n Q  
**Fecha**: 2024

