# Q-LEARNING-8-PUZZLE

Este es un repositorio colaborativo para la realizaci√≥n de una tarea sobre **Aplicaci√≥n de Aprendizaje por Refuerzo con Q-Learning** para resolver el **Problema del 8-Puzzle** en el contexto de Programaci√≥n en IA.

## üìã Descripci√≥n de la Tarea

### Objetivos de Aprendizaje

- Comprender y aplicar los conceptos de aprendizaje por refuerzo y el algoritmo Q-learning
- Implementar un agente de aprendizaje por refuerzo que resuelva el problema del 8-puzzle
- Evaluar el rendimiento del agente y ajustar los hiperpar√°metros para mejorar la eficacia del aprendizaje

### 1. Introducci√≥n al Problema del 8-Puzzle

El problema del 8-puzzle consiste en una cuadr√≠cula de 3x3 con 8 piezas numeradas del 1 al 8 y un espacio vac√≠o. El objetivo es reordenar las piezas de modo que coincidan con un estado objetivo predefinido, por ejemplo:

```
1 2 3
4 5 6
7 8
```

### 2. Configuraci√≥n del Entorno de Aprendizaje

Implementa un entorno para el 8-puzzle donde el agente pueda mover las piezas en las cuatro direcciones posibles: arriba, abajo, izquierda y derecha. Aseg√∫rate de definir las reglas de movimiento (es decir, que no puede moverse fuera de los l√≠mites de la cuadr√≠cula) y de penalizar movimientos inv√°lidos.

### 3. Definici√≥n de Recompensas y Estados

- Cada estado es una configuraci√≥n √∫nica del tablero
- El objetivo del agente es llegar al estado objetivo
- Asigna una recompensa negativa para cada paso para incentivar al agente a resolver el puzzle en el menor n√∫mero de movimientos posible
- Otorga una recompensa positiva significativa al alcanzar el estado objetivo

### 4. Implementaci√≥n del Algoritmo Q-Learning

- Implementa la tabla Q para almacenar las recompensas para cada acci√≥n en cada estado
- Usa una pol√≠tica Œµ-greedy para equilibrar la exploraci√≥n y explotaci√≥n
- Aplica la actualizaci√≥n de la funci√≥n Q con la ecuaci√≥n:

```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
```

donde:
- `s` y `s'` son el estado actual y el siguiente
- `a` y `a'` son las acciones actuales y las posibles en `s'`
- `Œ±` es la tasa de aprendizaje
- `Œ≥` es el factor de descuento

### 5. Entrenamiento del Agente

- Entrena al agente con diferentes configuraciones iniciales del 8-puzzle
- Ajusta los hiperpar√°metros (Œµ, Œ±, Œ≥) para optimizar el rendimiento
- Registra el n√∫mero de pasos que toma el agente para resolver el puzzle desde diferentes estados

### 6. Evaluaci√≥n y An√°lisis de Resultados

- Eval√∫a el rendimiento del agente en el 8-puzzle
- Presenta gr√°ficas que muestren c√≥mo cambia la tasa de √©xito y el n√∫mero de pasos promedio para resolver el puzzle a lo largo del tiempo
- Reflexiona sobre el impacto de cada hiperpar√°metro en el rendimiento del agente y su capacidad de generalizaci√≥n desde estados iniciales aleatorios

### 7. Entrega

- C√≥digo implementado con explicaciones de cada parte
- Un informe que incluya:
  - Gr√°ficas de rendimiento
  - An√°lisis de los resultados
  - Reflexiones sobre los desaf√≠os encontrados y las decisiones de dise√±o tomadas

## üìä Criterios de Evaluaci√≥n

- **Estructura, funcionamiento correcto y claridad del c√≥digo** (3 Puntos)
- **Documentaci√≥n y explicaciones** (3 Puntos)
- **Calidad del an√°lisis de resultados** (2 Puntos)
- **Ajuste y justificaci√≥n de los hiperpar√°metros** (2 Puntos)

## üí° Ayuda para la Implementaci√≥n

### Generaci√≥n de la Tabla de Transiciones

Definimos la tabla de transiciones generando todos los estados posibles como permutaciones del n√∫mero `123456789` siendo el hueco el n√∫mero 9.

Los estados ir√≠an desde el estado soluci√≥n `123456789` hasta el `987654321`.

Para generar la tabla sabemos que hay cuatro posibles acciones que puede realizar el hueco:
- `a0` (subir)
- `a1` (derecha)
- `a2` (abajo)
- `a3` (izquierda)

Partiendo del estado soluci√≥n no todos los estados son alcanzables.

Realizamos un proceso sistem√°tico generando un array bidimensional `T` de `(987654321 - 123456789)` filas y 4 columnas rellenadas a `-1`. Una vez creada recorremos cada fila y columna y vamos rellenando la tabla con los estados alcanzables. (Se generan dos grafos no conexos de la mitad de nodos cada uno). Con un recorrido en anchura desde el estado soluci√≥n podr√≠a marcar los alcanzables.
